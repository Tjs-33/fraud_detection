{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.linalg import eigh\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for credit card frauds\n",
    "raw_data = pd.read_csv('dataset/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style='font-size:20px'><b> Common data preprocessing steps </b></p>\n",
    "<p>\n",
    "\n",
    "1. PCA transformation: As seen during EDA, all features are already PCA transformed & contribute equally for explaining the cariance in data. Thereby, there is no scope of feature selection/shortlisting through PCA.\n",
    "2. Null values imputation: No null values were seen in the dataset.     \n",
    "3. Data balancing: This may be required since we have a highly imbalanced dataset. However, this will be implemented by assigning class weights while modelling, rather than changing the data itself.\n",
    "4. Column Standardisation: This will be done separately for each level of modelling as will be mentioned later.\n",
    "5. Feature modification: Instead of using the time in sec as a feature, we will simply use the hour number as feature considering practical intuition that minutes & seconds wouldn't add value beyond the hour number.\n",
    "6. Train test split - We will split the data in a 75:25 split in a way that the ratio between fraud:not-fraud datapoints remains similar in the train & test data\n",
    "    \n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_to_hms(time_in_sec):\n",
    "    \n",
    "    h = int(time_in_sec//3600)\n",
    "    m = int((time_in_sec%3600)//60)\n",
    "    s = int((time_in_sec%3600)%60)\n",
    "    return (h,m,s)\n",
    "\n",
    "raw_data.sort_values('Time', inplace = True)\n",
    "temp = raw_data.Time.apply(sec_to_hms)\n",
    "raw_data['Hour_num'] = [x[0] for x in temp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in training and test - 75:25 split\n",
    "\n",
    "train, test = train_test_split(raw_data, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Dataset      # Total pts     # pos pts      # neg pts      % pos pts      % neg pts   \n",
      "\n",
      "     Train          213605          373           213232        0.1746%        99.8254%   \n",
      "\n",
      "      Test          71202           119           71083         0.1671%        99.8329%   \n",
      "\n",
      "     Total          284807          492           284315        0.1727%        99.8273%   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution accross train & test data\n",
    "\n",
    "train_pos = train[(train.Class == 1)].shape[0]\n",
    "train_neg = train[(train.Class == 0)].shape[0]\n",
    "train_total = train_pos + train_neg\n",
    "test_pos = test[(test.Class == 1)].shape[0]\n",
    "test_neg = test[(test.Class == 0)].shape[0]\n",
    "test_total = test_neg + test_pos\n",
    "\n",
    "to_print = []\n",
    "\n",
    "to_print.append(['Dataset', '# Total pts', '# pos pts', '# neg pts', '% pos pts', '% neg pts'])\n",
    "to_print.append(['Train', train_total, train_pos, train_neg, train_pos/train_total, train_neg/train_total])\n",
    "to_print.append(['Test', test_total, test_pos, test_neg, test_pos/test_total, test_neg/test_total])\n",
    "to_print.append(['Total', train_total + test_total, train_pos + test_pos, train_neg + test_neg, (train_pos + test_pos)/(train_total + test_total), (train_neg + test_neg)/(train_total + test_total)])\n",
    "\n",
    "col_width = 15\n",
    "\n",
    "for item in to_print[0]:\n",
    "    print(item.center(col_width), end = \"\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for row in to_print[1:]:\n",
    "\n",
    "    col = []\n",
    "    col.append(row[0])\n",
    "    col.append(str(row[1]))\n",
    "    col.append(str(row[2]))\n",
    "    col.append(str(row[3]))\n",
    "    col.append(str(round(100*row[4],4)) + '%')\n",
    "    col.append(str(round(100*row[5],4)) + '%')\n",
    "    \n",
    "    for i in range(len(col)):\n",
    "        print(col[i].center(col_width), end = \"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "The data has been split in train, test with each dataset containin ~0.17% of positive class points.\n",
    "We will proceed to save this as train, test data.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train.to_csv('dataset/creditcard_train.csv', mode = 'x', index = False)\n",
    "except:\n",
    "    print(\"File already saved?\")\n",
    "\n",
    "try:\n",
    "    test.to_csv('dataset/creditcard_test.csv', mode = 'x', index = False)\n",
    "except:\n",
    "    print(\"File already saved?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
