{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "import datetime\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style='font-size:20px'><b> Modelling approach </b></p>\n",
    "<p>\n",
    "\n",
    "- Cascading ensemble has been proven quite effective for solving classification problems with highly imbalanced datasets & the same will be used for this project.\n",
    "- We will develop 3 layers of model, in each layer certain # of points will be filtered for next level and the rest will be cleared as 'not-fraud' in that level. \n",
    "- The # of points passed to the next level will be decided by setting a threshold on the predicted probability of a point being fraud.     \n",
    "- This will enable us to tune thresholds as per the business requirement of the model as mentioned during EDA.\n",
    "    \n",
    "</p>\n",
    "\n",
    "<p style='font-size:18px'><b> 1) Level 1 Model </b></p>\n",
    "<p style='font-size:16px'><b> 1.1 Objective </b></p>\n",
    "\n",
    "<p>\n",
    "\n",
    "- As visualised in EDA, there is a huge/dense cluster of not-fraud points that can be easily separated from the overlapping region of the two classes.\n",
    "- The objective of this level will be to classify these easily separable points as 'not-fraud' & pass rest of the points to the next level.     \n",
    "    \n",
    "</p>\n",
    "\n",
    "<p style='font-size:16px'><b> 1.2 Model charecteristics </b></p>\n",
    "\n",
    "<p>\n",
    "\n",
    "Based on the objective of this level, the ML model should have the following properties:\n",
    "- Doesn't need to have high complexity, since we are only targeting the easily separable not-fraud points.\n",
    "- Should have a good probabilistic interpretation, to enable tuning of the probability threshold.\n",
    "- For practical purposes, a fast run-time would be beneficial to green flag transactions that are very likely to be not-fraud.\n",
    "    \n",
    "Some models that can suit these characteristics:\n",
    "1. Logistic regression    \n",
    "2. RBF SVM    \n",
    "3. Decision tree\n",
    "    \n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for credit card frauds\n",
    "train_data = pd.read_csv('dataset/creditcard_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:18px'><b> 2) Custom function: Probability thresholding</b></p>\n",
    "\n",
    "<p>\n",
    "\n",
    "Following are the details of this function (input, output, purpose)\n",
    "    \n",
    "A) Inputs:\n",
    "- y_proba: The prob value predicted by the model for the dataset\n",
    "- y_actual: The actual class label of corresponding points in the the dataset\n",
    "- num_buckets: The number of equally spaced buckets of probability values\n",
    "\n",
    "B) OUTPUT:\n",
    "- The num of positive & negative class labels that fall under probability range buckets defined by num_buckets\n",
    "- The lowest probability value(threshold) that a positive (fraud) class label can assume accross all predictions\n",
    "- The number of points below 100%, 90%, 50% value of this threshold\n",
    "\n",
    "C) PURPOSE:\n",
    "- To decide the threshold for qualifying a point as not-fraud with high confidence based on probability values.\n",
    "- The points below this threshold will 'clear' level 1 to be classified as 'not-fraud', rest of the points will be passed as training data to the next level.\n",
    "- We will set this threshold to be 25% lower than the lowest probability value in the positive class label.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_class(y_proba, y_actual, num_buckets):\n",
    "    \n",
    "    prob_buckets = np.linspace(0,1,num_buckets+1)\n",
    "    prob_class_report = []\n",
    "    prob_class_report.append(['Prob range', 'Not-Fraud', 'Fraud'])\n",
    "    \n",
    "    for i in range(len(prob_buckets)-1):\n",
    "        \n",
    "        # Col1: Probability range\n",
    "        prob_range = '(' + str(round(prob_buckets[i],2)) + ', ' + str(round(prob_buckets[i+1],2)) + ']'\n",
    "        \n",
    "        # Defining the upper and lower bound of the probability\n",
    "        prob_high = prob_buckets[i+1]\n",
    "        if i==0:\n",
    "            prob_low = -0.01\n",
    "        else:\n",
    "            prob_low = prob_buckets[i]\n",
    "        \n",
    "        # Col2: #Points that are not fraud in the prob_range\n",
    "        not_fraud = np.sum((y_proba>prob_low)&(y_proba<=prob_high)&(y_actual==0))\n",
    "\n",
    "        # Col3: #Points that are fraud in the prob_range\n",
    "        fraud = np.sum((y_proba>prob_low)&(y_proba<=prob_high)&(y_actual==1))\n",
    "        \n",
    "        prob_class_report.append([prob_range, not_fraud, fraud])\n",
    "    \n",
    "    # Printing the Prob range vs count of fraud or not-fraud points\n",
    "    print(\"\\nDistribution of class labels for various prob buckets: \\n\")\n",
    "    col_width = 15\n",
    "    print(\"\".join(word.center(col_width) for word in prob_class_report[0]))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    for row in prob_class_report[1:]:\n",
    "        print(str(row[0]).center(col_width), end = \"\")\n",
    "        print(str(row[1]).center(col_width), end = \"\")\n",
    "        print(str(row[2]).center(col_width))\n",
    "    \n",
    "    # Prob value of the fraud datapoint having lowest prob value\n",
    "    fraud_min_prob = np.min(y_proba[(y_actual==1)])\n",
    "    \n",
    "    # Number of not fraud datapoints having prob value below the fraud_min_prob\n",
    "    not_fraud_1 = np.sum((y_actual==0)&(y_proba<fraud_min_prob))\n",
    "\n",
    "    # Number of not fraud datapoints having prob value below the 90% of fraud_min_prob\n",
    "    not_fraud_2 = np.sum((y_actual==0)&(y_proba<0.9*fraud_min_prob))\n",
    "    \n",
    "    # Number of not fraud datapoints having prob value below the 90% of fraud_min_prob\n",
    "    not_fraud_3 = np.sum((y_actual==0)&(y_proba<0.5*fraud_min_prob))\n",
    "    \n",
    "    print(\"\\n1. Lowest prob value in the fraud class (threshold): {}%\".format(round(100*fraud_min_prob,3)))\n",
    "    print(\"2. # of not fraud pts below threshold: {}\".format(not_fraud_1))\n",
    "    print(\"3. # of not fraud pts below 90% of threshold: {}\".format(not_fraud_2))\n",
    "    print(\"4. # of not fraud pts below 50% of threshold: {}\".format(not_fraud_3))\n",
    "    \n",
    "    return (fraud_min_prob, not_fraud_1, not_fraud_2, not_fraud_3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X (input) & Y (output)\n",
    "train_x = train_data.drop(['Time', 'Class'], axis = 1).to_numpy()\n",
    "train_y = train_data['Class'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data - not necessary for Decision trees, but required for Logistic regression & RBF SVM\n",
    "scaler = StandardScaler()\n",
    "train_x_scaled = scaler.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style='font-size:18px'><b> Option 1: Logistic regression </b></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0:01:37.478292\n",
      "Hyper param values for best estimator:  {'C': 0.1, 'class_weight': {0: 1, 1: 100}}\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "logreg_ = LogisticRegression(max_iter=1000)\n",
    "hyper_params = {'C': [10**-2, 10**-1, 10**0, 10**1, 10**2], 'class_weight' : [{0:1, 1:1}, {0:1, 1:5}, {0:1, 1:10}, {0:1, 1:100}]}\n",
    "logreg_gcv = GridSearchCV(logreg_, hyper_params, scoring = 'recall', cv = 5)\n",
    "logreg_gcv.fit(train_x_scaled, train_y)\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - start_time)\n",
    "print(\"Hyper param values for best estimator: \", logreg_gcv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of class labels for various prob buckets: \n",
      "\n",
      "   Prob range     Not-Fraud        Fraud     \n",
      "----------------------------------------------------\n",
      "  (0.0, 0.05]       195010           17      \n",
      "  (0.05, 0.1]       10151            12      \n",
      "  (0.1, 0.15]        3772            4       \n",
      "  (0.15, 0.2]        1600            3       \n",
      "  (0.2, 0.25]        763             1       \n",
      "  (0.25, 0.3]        379             1       \n",
      "  (0.3, 0.35]        251             4       \n",
      "  (0.35, 0.4]        239             2       \n",
      "  (0.4, 0.45]        206             1       \n",
      "  (0.45, 0.5]        160             2       \n",
      "  (0.5, 0.55]        150             2       \n",
      "  (0.55, 0.6]        135             2       \n",
      "  (0.6, 0.65]         85             1       \n",
      "  (0.65, 0.7]         64             2       \n",
      "  (0.7, 0.75]         43             2       \n",
      "  (0.75, 0.8]         46             2       \n",
      "  (0.8, 0.85]         27             4       \n",
      "  (0.85, 0.9]         29             1       \n",
      "  (0.9, 0.95]         26             3       \n",
      "  (0.95, 1.0]         96            307      \n",
      "\n",
      "1. Lowest prob value in the fraud class (threshold): 0.325%\n",
      "2. # of not fraud pts below threshold: 49007\n",
      "3. # of not fraud pts below 90% of threshold: 43574\n",
      "4. # of not fraud pts below 50% of threshold: 21552\n",
      "\n",
      "Prob threshold selected: 0.244%\n",
      "Number of points filtered for L2:  178156\n"
     ]
    }
   ],
   "source": [
    "# Probability vs class labels\n",
    "l1_logreg_clf = logreg_gcv.best_estimator_\n",
    "y_pred_proba_train = l1_logreg_clf.predict_proba(train_x_scaled)[:,1]\n",
    "prob_details = prob_class(y_pred_proba_train, train_y, 20)\n",
    "\n",
    "# Filter the points that we are not confident to be not-fraud\n",
    "prob_thresh = prob_details[0]*0.75\n",
    "print(\"\\nProb threshold selected: {}%\".format(round(100*prob_thresh,3)))\n",
    "temp = train_data[(y_pred_proba_train>=prob_thresh)]\n",
    "print(\"Number of points filtered for L2: \", temp.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p style='font-size:18px'><b> Option 2: RBF SVM </b></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0:03:44.426800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Option 2: RBF SVM\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "class_weights = {0:1, 1:5}\n",
    "l1_rbfsvm_clf = SVC(C=1.0, max_iter=5000, kernel='rbf', probability=True, class_weight=class_weights)\n",
    "l1_rbfsvm_clf.fit(train_x_scaled, train_y)\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of class labels for various prob buckets: \n",
      "\n",
      "   Prob range     Not-Fraud        Fraud     \n",
      "----------------------------------------------------\n",
      "  (0.0, 0.05]       213209           30      \n",
      "  (0.05, 0.1]         4              4       \n",
      "  (0.1, 0.15]         5              2       \n",
      "  (0.15, 0.2]         0              1       \n",
      "  (0.2, 0.25]         1              1       \n",
      "  (0.25, 0.3]         1              0       \n",
      "  (0.3, 0.35]         0              2       \n",
      "  (0.35, 0.4]         0              0       \n",
      "  (0.4, 0.45]         0              0       \n",
      "  (0.45, 0.5]         2              0       \n",
      "  (0.5, 0.55]         0              0       \n",
      "  (0.55, 0.6]         1              0       \n",
      "  (0.6, 0.65]         0              0       \n",
      "  (0.65, 0.7]         1              0       \n",
      "  (0.7, 0.75]         0              2       \n",
      "  (0.75, 0.8]         1              0       \n",
      "  (0.8, 0.85]         0              0       \n",
      "  (0.85, 0.9]         1              1       \n",
      "  (0.9, 0.95]         0              3       \n",
      "  (0.95, 1.0]         6             327      \n",
      "\n",
      "1. Lowest prob value in the fraud class (threshold): 0.254%\n",
      "2. # of not fraud pts below threshold: 200046\n",
      "3. # of not fraud pts below 90% of threshold: 194719\n",
      "4. # of not fraud pts below 50% of threshold: 145754\n",
      "\n",
      "Prob threshold selected: 0.191%\n",
      "Number of points filtered for L2:  30669\n"
     ]
    }
   ],
   "source": [
    "# Probability vs class labels\n",
    "y_pred_proba_train = l1_rbfsvm_clf.predict_proba(train_x_scaled)[:,1]\n",
    "prob_details = prob_class(y_pred_proba_train, train_y, 20)\n",
    "\n",
    "# Filter the points that we are not confident to be not-fraud\n",
    "prob_thresh = prob_details[0]*0.75\n",
    "print(\"\\nProb threshold selected: {}%\".format(round(100*prob_thresh,3)))\n",
    "temp = train_data[(y_pred_proba_train>=prob_thresh)]\n",
    "print(\"Number of points filtered for L2: \", temp.shape[0])\n",
    "\n",
    "l1_prob_thresh = prob_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p style='font-size:18px'><b> Option 3: Decision tree </b></p>\n",
    "\n",
    "\n",
    "<p>\n",
    "\n",
    "- Decision trees can easily overfit the data, we need to constraint the model to a shallow decision tree.\n",
    "- The leaf nodes having all not-fraud datapoints will be cleared from this level, and the nodes having both class labels will be filtered for next level.    \n",
    "- Therefore, we will experiment with various depths to choose the best model with the right bias-variance tradeoff.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0:00:03.822064\n",
      "\n",
      "1. Max depth:  5\n",
      "2. # of nodes:  41\n"
     ]
    }
   ],
   "source": [
    "# Decision tree 1: Max depth = 5\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "l1_dectree_clf1 = DecisionTreeClassifier(max_depth = 5)\n",
    "l1_dectree_clf1.fit(train_x_scaled, train_y)\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - start_time)\n",
    "\n",
    "# Some attributes of the decision tree\n",
    "print(\"\\n1. Max depth: \", l1_dectree_clf1.tree_.max_depth)\n",
    "print(\"2. # of nodes: \", l1_dectree_clf1.tree_.node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of class labels for various prob buckets: \n",
      "\n",
      "   Prob range     Not-Fraud        Fraud     \n",
      "----------------------------------------------------\n",
      "  (0.0, 0.05]       213180           58      \n",
      "  (0.05, 0.1]         26             2       \n",
      "  (0.1, 0.15]         0              0       \n",
      "  (0.15, 0.2]         8              2       \n",
      "  (0.2, 0.25]         0              0       \n",
      "  (0.25, 0.3]         0              0       \n",
      "  (0.3, 0.35]         0              0       \n",
      "  (0.35, 0.4]         0              0       \n",
      "  (0.4, 0.45]         0              0       \n",
      "  (0.45, 0.5]         0              0       \n",
      "  (0.5, 0.55]         0              0       \n",
      "  (0.55, 0.6]         6              9       \n",
      "  (0.6, 0.65]         0              0       \n",
      "  (0.65, 0.7]         0              0       \n",
      "  (0.7, 0.75]         0              0       \n",
      "  (0.75, 0.8]         0              0       \n",
      "  (0.8, 0.85]         1              5       \n",
      "  (0.85, 0.9]         0              0       \n",
      "  (0.9, 0.95]         7              90      \n",
      "  (0.95, 1.0]         4             207      \n",
      "\n",
      "1. Lowest prob value in the fraud class (threshold): 0.025%\n",
      "2. # of not fraud pts below threshold: 49\n",
      "3. # of not fraud pts below 90% of threshold: 49\n",
      "4. # of not fraud pts below 50% of threshold: 49\n",
      "\n",
      "Prob threshold selected: 0.019%\n",
      "Number of points filtered for L2:  213556\n"
     ]
    }
   ],
   "source": [
    "# Probability vs class labels\n",
    "y_pred_proba_train = l1_dectree_clf1.predict_proba(train_x_scaled)[:,1]\n",
    "prob_details = prob_class(y_pred_proba_train, train_y, 20)\n",
    "\n",
    "# Filter the points that we are not confident to be not-fraud\n",
    "prob_thresh = prob_details[0]*0.75\n",
    "print(\"\\nProb threshold selected: {}%\".format(round(100*prob_thresh,3)))\n",
    "temp = train_data[(y_pred_proba_train>=prob_thresh)]\n",
    "print(\"Number of points filtered for L2: \", temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0:00:09.893523\n",
      "\n",
      "1. Max depth:  15\n",
      "2. # of nodes:  225\n"
     ]
    }
   ],
   "source": [
    "# Decision tree 2: Max depth = 15\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "l1_dectree_clf2 = DecisionTreeClassifier(max_depth = 15)\n",
    "l1_dectree_clf2.fit(train_x_scaled, train_y)\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - start_time)\n",
    "\n",
    "# Some attributes of the decision tree\n",
    "print(\"\\n1. Max depth: \", l1_dectree_clf2.tree_.max_depth)\n",
    "print(\"2. # of nodes: \", l1_dectree_clf2.tree_.node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of class labels for various prob buckets: \n",
      "\n",
      "   Prob range     Not-Fraud        Fraud     \n",
      "----------------------------------------------------\n",
      "  (0.0, 0.05]       213213           15      \n",
      "  (0.05, 0.1]         9              1       \n",
      "  (0.1, 0.15]         0              0       \n",
      "  (0.15, 0.2]         0              0       \n",
      "  (0.2, 0.25]         10             3       \n",
      "  (0.25, 0.3]         0              0       \n",
      "  (0.3, 0.35]         0              0       \n",
      "  (0.35, 0.4]         0              0       \n",
      "  (0.4, 0.45]         0              0       \n",
      "  (0.45, 0.5]         0              0       \n",
      "  (0.5, 0.55]         0              0       \n",
      "  (0.55, 0.6]         0              0       \n",
      "  (0.6, 0.65]         0              0       \n",
      "  (0.65, 0.7]         0              0       \n",
      "  (0.7, 0.75]         0              0       \n",
      "  (0.75, 0.8]         0              0       \n",
      "  (0.8, 0.85]         0              0       \n",
      "  (0.85, 0.9]         0              0       \n",
      "  (0.9, 0.95]         0              0       \n",
      "  (0.95, 1.0]         0             354      \n",
      "\n",
      "1. Lowest prob value in the fraud class (threshold): 0.006%\n",
      "2. # of not fraud pts below threshold: 166352\n",
      "3. # of not fraud pts below 90% of threshold: 166352\n",
      "4. # of not fraud pts below 50% of threshold: 166352\n",
      "\n",
      "Prob threshold selected: 0.004%\n",
      "Number of points filtered for L2:  47253\n"
     ]
    }
   ],
   "source": [
    "# Probability vs class labels\n",
    "y_pred_proba_train = l1_dectree_clf2.predict_proba(train_x_scaled)[:,1]\n",
    "prob_details = prob_class(y_pred_proba_train, train_y, 20)\n",
    "\n",
    "# Filter the points that we are not confident to be not-fraud\n",
    "prob_thresh = prob_details[0]*0.75\n",
    "print(\"\\nProb threshold selected: {}%\".format(round(100*prob_thresh,3)))\n",
    "temp = train_data[(y_pred_proba_train>=prob_thresh)]\n",
    "print(\"Number of points filtered for L2: \", temp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0:00:10.234272\n",
      "\n",
      "1. Max depth:  19\n",
      "2. # of nodes:  287\n"
     ]
    }
   ],
   "source": [
    "# Decision tree 3: Max depth = 19\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "l1_dectree_clf3 = DecisionTreeClassifier(max_depth = 21)\n",
    "l1_dectree_clf3.fit(train_x_scaled, train_y)\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - start_time)\n",
    "\n",
    "# Some attributes of the decision tree\n",
    "print(\"\\n1. Max depth: \", l1_dectree_clf3.tree_.max_depth)\n",
    "print(\"2. # of nodes: \", l1_dectree_clf3.tree_.node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of class labels for various prob buckets: \n",
      "\n",
      "   Prob range     Not-Fraud        Fraud     \n",
      "----------------------------------------------------\n",
      "  (0.0, 0.05]       213232           0       \n",
      "  (0.05, 0.1]         0              0       \n",
      "  (0.1, 0.15]         0              0       \n",
      "  (0.15, 0.2]         0              0       \n",
      "  (0.2, 0.25]         0              0       \n",
      "  (0.25, 0.3]         0              0       \n",
      "  (0.3, 0.35]         0              0       \n",
      "  (0.35, 0.4]         0              0       \n",
      "  (0.4, 0.45]         0              0       \n",
      "  (0.45, 0.5]         0              0       \n",
      "  (0.5, 0.55]         0              0       \n",
      "  (0.55, 0.6]         0              0       \n",
      "  (0.6, 0.65]         0              0       \n",
      "  (0.65, 0.7]         0              0       \n",
      "  (0.7, 0.75]         0              0       \n",
      "  (0.75, 0.8]         0              0       \n",
      "  (0.8, 0.85]         0              0       \n",
      "  (0.85, 0.9]         0              0       \n",
      "  (0.9, 0.95]         0              0       \n",
      "  (0.95, 1.0]         0             373      \n",
      "\n",
      "1. Lowest prob value in the fraud class (threshold): 100.0%\n",
      "2. # of not fraud pts below threshold: 213232\n",
      "3. # of not fraud pts below 90% of threshold: 213232\n",
      "4. # of not fraud pts below 50% of threshold: 213232\n",
      "\n",
      "Prob threshold selected: 75.0%\n",
      "Number of points filtered for L2:  373\n"
     ]
    }
   ],
   "source": [
    "# Probability vs class labels\n",
    "y_pred_proba_train = l1_dectree_clf3.predict_proba(train_x_scaled)[:,1]\n",
    "prob_details = prob_class(y_pred_proba_train, train_y, 20)\n",
    "\n",
    "# Filter the points that we are not confident to be not-fraud\n",
    "prob_thresh = prob_details[0]*0.75\n",
    "print(\"\\nProb threshold selected: {}%\".format(round(100*prob_thresh,3)))\n",
    "temp = train_data[(y_pred_proba_train>=prob_thresh)]\n",
    "print(\"Number of points filtered for L2: \", temp.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:18px'><b> Comparison </b></p>\n",
    "\n",
    "<p>\n",
    "\n",
    "1. Logistic regression\n",
    "- Has the fastest run-time, but is very simple & underfitting making it unable to clear a sufficient number of 'not-fraud' points from this level.\n",
    "\n",
    "\n",
    "2. RB SVM\n",
    "- Has the worst train & run time, but has the best probabilistic interpretation.\n",
    "- It is also able to clear a hige number of 'not-fraud' points in level 1 without overfitting the data.\n",
    "\n",
    "\n",
    "3. Decision tree: After experimenting with the depth of the decision tree, following observations were made:\n",
    "- For depth <= 20: The model was heavily underfitting allowing only ~374 points to clear the level 1 as not-fraud\n",
    "- For depth = 27: The model completely overfits the data will 100% accuracy, precision & recall\n",
    "- For depth IN [21, 24]: A balanced bias-varaince tradeoff is visible, with the model filtering ~15k points for level 2,similar to RBF SVM, but with a much faster train & run time.\n",
    "\n",
    "</p>\n",
    "\n",
    "\n",
    "<p style='font-size:16px'><b> Conclusion </b></p>\n",
    "    \n",
    "<p>\n",
    "    \n",
    "- We will choose RBF SVM as the final model for L1 due to it's better probabilistic interpretation.\n",
    "- The prob values on decision tree are simply the fraction of positive points in a leaf node, this may not work well enough for future unseen datapoints.\n",
    "- However, for faster run-times, we can experiment with decision tree separately.\n",
    "                     \n",
    "</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:18px'><b> Comparison </b></p>\n",
    "\n",
    "<p>\n",
    "\n",
    "\n",
    "A) Logistic regression\n",
    "- Has the fastest run-time, but is very simple & underfitting making it unable to clear a sufficient number of 'not-fraud' points from this level.\n",
    "    \n",
    "B) RBF SVM\n",
    "- Has the worst train & run time, but has the best probabilistic interpretation.\n",
    "- It is also able to clear a huge number of 'not-fraud' points in level 1 without overfitting the data.\n",
    "    \n",
    "C) Decision tree: After experimenting with various depths, following observations were made:\n",
    "- Below a certain depth value, the model was heavily underfitting allowing < 2000 points to clear the level 1 as not-fraud.    \n",
    "- Beyond another depth value, the model completely overfits train data with 100% accuracy, precision & recall.\n",
    "- Only when the depth is in a certain range, the model filters a sufficient number of datapoints & also doesn't overfit the dataset.                                                                                 \n",
    " \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style='font-size:18px'><b> Conclusion </b></p>\n",
    "\n",
    "<p>\n",
    "\n",
    "- We will choose RBF SVM as the final model for L1 due to it's better probabilistic interpretation.\n",
    "- The prob values on decision tree are simply the fraction of positive points in a leaf node, this may not work well enough for future unseen datapoints\n",
    "- However, for faster run-times, we can experiment with decision tree separately.\n",
    "\n",
    " \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/l1_prob_thresh.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering points that didn't pass the L1 model as 'not-fraud' to be used as training data for L2 model\n",
    "y_pred_proba_train = l1_rbfsvm_clf.predict_proba(train_x_scaled)[:,1]\n",
    "train_l2 = train_data[(y_pred_proba_train>=l1_prob_thresh)]\n",
    "\n",
    "try:\n",
    "    train_l2.to_csv('dataset/creditcard_train_l2.csv', mode = 'x', index = False)\n",
    "except:\n",
    "    print(\"File already saved?\")\n",
    "\n",
    "joblib.dump(scaler, 'models/l1_scaler.pkl')\n",
    "joblib.dump(l1_rbfsvm_clf, 'models/l1_rbfsvm_clf.pkl')\n",
    "joblib.dump(l1_prob_thresh, 'models/l1_prob_thresh.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
